# -*- coding: utf-8 -*-
"""

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12m7eAdoMR8gPAragRlx1Jf6A_lYSCUIO

"""  ## Imports"""

import copy
import math
import pickle
import random

# Regex
import re
import time

import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from scipy.integrate import odeint
from tqdm import trange

# Tell it to use GPU

if torch.cuda.is_available():
    device = torch.device("cuda")
    torch.set_default_tensor_type(torch.cuda.DoubleTensor)
    print("Using GPU")
else:
    device = torch.device("cpu")
    torch.set_default_tensor_type(torch.DoubleTensor)
    print("No GPU found, using cpu")

# Code to take the derivative with respect to the input.
def diff(u, t, order=1):
    # code adapted from neurodiffeq library
    # https://github.com/NeuroDiffGym/neurodiffeq/blob/master/neurodiffeq/neurodiffeq.py
    """The derivative of a variable with respect to another."""
    # ones = torch.ones_like(u)

    der = torch.cat(
        [
            torch.autograd.grad(u[:, i].sum(), t, create_graph=True)[0]
            for i in range(u.shape[1])
        ],
        1,
    )
    if der is None:
        print("derivative is None")
        return torch.zeros_like(t, requires_grad=True)
    else:
        der.requires_grad_()
    for i in range(1, order):

        der = torch.cat(
            [
                torch.autograd.grad(der[:, i].sum(), t, create_graph=True)[0]
                for i in range(der.shape[1])
            ],
            1,
        )
        # print()
        if der is None:
            print("derivative is None")
            return torch.zeros_like(t, requires_grad=True)
        else:
            der.requires_grad_()
    return der


"""## Defining the NN
"""


class MyNetwork_Ray_Tracing(nn.Module):
    """
    function to learn the hidden states derivatives hdot
    """

    def __init__(
        self, number_dims=40, number_dims_heads=10, depth_body=4, N=1, Number_heads_TL=1
    ):
        """number_dims is the number of nodes within each layer
        Actually depth_body is 1 minus the number of hidden layers
        N is the number of heads
        """
        super(MyNetwork_Ray_Tracing, self).__init__()
        self.N = N
        self.Number_heads_TL = Number_heads_TL
        self.depth_body = depth_body
        self.number_dims = number_dims
        # Tanh activation function
        self.nl = nn.Tanh()
        self.lin1 = nn.Linear(1, number_dims)
        self.lin2 = nn.ModuleList([nn.Linear(number_dims, number_dims)])
        self.lin2.extend(
            [nn.Linear(number_dims, number_dims) for i in range(depth_body - 1)]
        )
        self.lina = nn.ModuleList([nn.Linear(number_dims, number_dims_heads)])
        self.lina.extend(
            [nn.Linear(number_dims, number_dims_heads) for i in range(N - 1)]
        )
        # 4 outputs for x,y, p_x, p_y
        self.lout1 = nn.ModuleList([nn.Linear(number_dims_heads, 4, bias=True)])
        self.lout1.extend(
            [nn.Linear(number_dims_heads, 4, bias=True) for i in range(N - 1)]
        )

        ### FOR TL
        self.lina_TL = nn.ModuleList([nn.Linear(number_dims, number_dims_heads)])
        self.lina_TL.extend(
            [
                nn.Linear(number_dims, number_dims_heads)
                for i in range(Number_heads_TL - 1)
            ]
        )
        # 4 outputs for x,y, p_x, p_y
        self.lout1_TL = nn.ModuleList([nn.Linear(number_dims_heads, 4, bias=True)])
        self.lout1_TL.extend(
            [
                nn.Linear(number_dims_heads, 4, bias=True)
                for i in range(Number_heads_TL - 1)
            ]
        )

    def base(self, t):
        x = self.lin1(t)
        x = self.nl(x)
        for m in range(self.depth_body):
            x = self.lin2[m](x)
            x = self.nl(x)
        return x

    def forward_initial(self, x):
        d = {}
        for n in range(self.N):
            xa = self.lina[n](x)
            d[n] = self.lout1[n](xa)
        return d

    def forward_TL(self, x):
        d = {}
        for n in range(self.Number_heads_TL):
            xa = self.lina_TL[n](x)
            d[n] = self.lout1_TL[n](xa)
        return d


"""## Numerical - copied to be the same as in Ray_TracingwithSavingandRandomSamplingansEnergy_forFeb15"""
lineW = 3
lineBoxW = 2
font = {"size": 24}

plt.rc("font", **font)
# plt.rcParams['text.usetex'] = True

# Use below in the Scipy Solver
def f_general(u, t, means_Gaussians, lam=1, sig=0.1, A_=0.1):
    # unpack current values of u
    x, y, px, py = u

    V = 0
    Vx = 0
    Vy = 0

    A = A_

    for i in means_Gaussians:
        muX1 = i[0]
        muY1 = i[1]
        V += -A * np.exp(-(((x - muX1) ** 2 + (y - muY1) ** 2) / sig**2) / 2)
        Vx += (
            A
            * np.exp(-(((x - muX1) ** 2 + (y - muY1) ** 2) / sig**2) / 2)
            * (x - muX1)
            / sig**2
        )
        Vy += (
            A
            * np.exp(-(((x - muX1) ** 2 + (y - muY1) ** 2) / sig**2) / 2)
            * (y - muY1)
            / sig**2
        )

    # derivatives of x, y, px, py
    derivs = [px, py, -Vx, -Vy]

    return derivs


# Scipy Solver
def rayTracing_general(t, x0, y0, px0, py0, means_Gaussians, lam=1, sig=0.1, A_=0.1):
    u0 = [x0, y0, px0, py0]
    # Call the ODE solver
    solPend = odeint(
        f_general,
        u0,
        t,
        args=(
            means_Gaussians,
            lam,
            sig,
            A_,
        ),
    )
    xP = solPend[:, 0]
    yP = solPend[:, 1]
    pxP = solPend[:, 2]
    pyP = solPend[:, 3]
    return xP, yP, pxP, pyP


"""## Experiments

### Sums of Gaussians

Now general case of n Gaussians, with the possibility of energy conservation.
"""

means_cell = [
    [0.74507886, 0.3602802],
    [0.40147605, 0.06139579],
    [0.94162198, 0.46722697],
    [0.79110703, 0.8973808],
    [0.64732527, 0.07095655],
    [0.10083943, 0.31935057],
    [0.24929806, 0.60499613],
    [0.11377013, 0.42598647],
    [0.85163671, 0.26495608],
    [0.18439795, 0.31438099],
]


def TL_N_heads_run_Gaussiann_transfer(
    network_in,
    specify_initial_condition=True,
    init_specified=0.55,
    step_LR_step=1000,
    step_LR_gamma=0.9,
    adam_lr=0.01,
    sgd_momentum=0,
    sgd_lr=0.005,
    energy_TL_weight=3,
    use_SGD_TL=False,
    parametrisation=True,
    max_grid_grow=400,
    TL_weighting=1,
    scale=0.7,
    A_=0.1,
    sig=0.1,
    initial_x=0,
    final_t=1,
    means=means_cell,
    alpha_=1,
    width_=40,
    width_heads=10,
    epochs_=25000,
    epochs_TL=25000,
    grid_size=400,
    number_of_heads=11,
    number_of_heads_TL=11,
    PATH="models",
    print_legend=False,
    loadWeights=False,
    energy_conservation=False,
    norm_clipping=False,
):

    network2 = copy.deepcopy(network_in)
    # Set out tensor of times
    t = torch.linspace(0, final_t, grid_size, requires_grad=True).reshape(-1, 1)

    # Number of epochs
    num_epochs = epochs_

    # We keep a log of the loss as a fct of the number of epochs
    loss_log = np.zeros(num_epochs)

    # Create a figure
    f, ax = plt.subplots(3, 1, figsize=(20, 80))

    # For comparaison
    temp_loss = np.inf

    for i in range(1):
        #################################################################
        #################################################################
        ###############   Tranfer learning ##############################
        #################################################################
        #################################################################
        t0_TL = time.time()

        # For comparaison
        temp_loss_TL = np.inf

        # Number of epochs
        num_epochs_TL = epochs_TL

        loss_log_TL = np.zeros(num_epochs_TL)

        ## LOADING WEIGHTS PART if PATH file exists and loadWeights=True
        print("We load the previous model for transfer learning")

        # checkpoint=torch.load(PATH)
        # https://pytorch.org/tutorials/beginner/saving_loading_models.html
        network50 = copy.deepcopy(network2)
        """
      network50=MyNetwork_Ray_Tracing(number_dims=width_, number_dims_heads=width_heads, N=number_of_heads,  Number_heads_TL=number_of_heads_TL)
      if not use_SGD_TL:
        optimizer50=optim.Adam(network50.parameters(), lr=1e-3)
      if use_SGD_TL:
        optimizer50=optim.SGD(network50.parameters(), momentum=0, lr=1e-2)
      """
        if not use_SGD_TL:
            optimizer50 = optim.Adam(network50.parameters(), lr=adam_lr)
        if use_SGD_TL:
            optimizer50 = optim.SGD(
                network50.parameters(), momentum=sgd_momentum, lr=sgd_lr
            )

        scheduler = torch.optim.lr_scheduler.StepLR(
            optimizer50, step_size=step_LR_step, gamma=step_LR_gamma
        )
        # device = torch.device("cuda")
        # network50.load_state_dict(checkpoint['model_state_dict'])
        # network50.to(device)
        # total_epochs=checkpoint['total_epochs']
        # print("We previously trained for {} epochs".format(total_epochs))
        # print('The loss was:', checkpoint['loss'], 'achieved at epoch', checkpoint['epoch'])
        # print("Now we are going to freeze some layers and keep training after that.")

        for name, param in network50.named_parameters():
            # print("param", name)
            # Use regex
            if re.match("lin1\..*", name):
                print("Freezing", name)
                param.requires_grad = False
            # Use regex
            if re.match("lin2\..*", name):
                print("Freezing", name)
                param.requires_grad = False

        # Dictionary for the initial conditions
        ic_TL = {}
        # Dictionary for the initial energy for each initial conditions
        H0_init_TL = {}

        if not specify_initial_condition:
            for j in range(number_of_heads_TL):
                # Initial conditions
                initial_condition_TL = random.randint(0, 100) / 100
                print(
                    "The initial condition (for y) for TL is {}".format(
                        initial_condition_TL
                    )
                )
                ic_TL[j] = initial_condition_TL
        else:
            for j in range(number_of_heads_TL):
                # Initial conditions
                initial_condition_TL = init_specified[j]
                print(
                    "The initial condition (for y) for TL is {}".format(
                        initial_condition_TL
                    )
                )
                ic_TL[j] = initial_condition_TL

        # Keep track of the number of epochs
        total_epochs_TL = 0

        # Dictionary keeping track of the loss for each head
        losses_part_TL = {}
        for k in range(number_of_heads_TL):
            losses_part_TL[k] = np.zeros(num_epochs_TL)

        # For every epoch...

        # NO Random sampling! No need to sample every epoch!
        t = torch.linspace(0, final_t, grid_size, requires_grad=True)
        t = t.reshape(-1, 1)

        optimizer50.zero_grad()
        x_base_TL = network50.base(t)
        with trange(num_epochs_TL) as tepoch_TL:
            for ne in tepoch_TL:
                tepoch_TL.set_description(f"Epoch {ne}")
                if ne > 0:
                    optimizer50.zero_grad()
                # Forward pass through the network
                # x_base_TL = network50.base(t)
                d_TL = network50.forward_TL(x_base_TL)
                # loss
                loss_TL = 0
                # for saving the best loss (of individual heads)
                losses_part_current_TL = {}

                # For each head...
                for l in range(number_of_heads_TL):
                    # Get the current head
                    head_TL = d_TL[l]
                    # Get the corresponding initial condition
                    initial_y_TL = ic_TL[l]

                    # Outputs
                    if parametrisation:
                        x_TL = initial_x + (1 - torch.exp(-t)) * head_TL[:, 0].reshape(
                            (-1, 1)
                        )
                        y_TL = initial_y_TL + (1 - torch.exp(-t)) * head_TL[
                            :, 1
                        ].reshape((-1, 1))
                        px_TL = 1 + (1 - torch.exp(-t)) * head_TL[:, 2].reshape((-1, 1))
                        py_TL = 0 + (1 - torch.exp(-t)) * head_TL[:, 3].reshape((-1, 1))
                    elif not parametrisation:
                        x_TL = head_TL[:, 0]
                        y_TL = head_TL[:, 1]
                        px_TL = head_TL[:, 2]
                        py_TL = head_TL[:, 3]
                    x_TL = x_TL.reshape((-1, 1))
                    y_TL = y_TL.reshape((-1, 1))
                    px_TL = px_TL.reshape((-1, 1))
                    py_TL = py_TL.reshape((-1, 1))
                    # Derivatives
                    x_dot_TL = diff(x_TL, t, 1)
                    y_dot_TL = diff(y_TL, t, 1)
                    px_dot_TL = diff(px_TL, t, 1)
                    py_dot_TL = diff(py_TL, t, 1)

                    # Loss
                    L1_TL = ((x_dot_TL - px_TL) ** 2).mean()
                    L2_TL = ((y_dot_TL - py_TL) ** 2).mean()

                    # For the other components of the loss, we need the potential V
                    # and its derivatives
                    ## Partial derivatives of the potential (updated below)
                    partial_x_TL = 0
                    partial_y_TL = 0

                    ## Energy at the initial time (updated below)
                    H_0_TL = 1 / 2
                    H_curr_TL = (px_TL**2 + py_TL**2) / 2

                    # VECTORIZE THIS
                    for i in range(len(means)):
                        # Get the current means
                        mu_x = means[i][0]
                        mu_y = means[i][1]

                        # Building the potential and updating the partial derivatives
                        potential_TL = -A_ * torch.exp(
                            -(1 / (2 * sig**2))
                            * ((x_TL - mu_x) ** 2 + (y_TL - mu_y) ** 2)
                        )
                        # Partial wrt to x
                        partial_x_TL += -potential_TL * (x_TL - mu_x) * (1 / (sig**2))
                        # Partial wrt to y
                        partial_y_TL += -potential_TL * (y_TL - mu_y) * (1 / (sig**2))

                        # Updating the energy
                        H_0_TL += -A_ * math.exp(
                            -(1 / (2 * sig**2))
                            * ((initial_x - mu_x) ** 2 + (initial_y_TL - mu_y) ** 2)
                        )
                        H_curr_TL += -A_ * torch.exp(
                            -(1 / (2 * sig**2))
                            * ((x_TL - mu_x) ** 2 + (y_TL - mu_y) ** 2)
                        )

                    ## We can finally set the energy for head l
                    H0_init_TL[l] = H_0_TL

                    # Other components of the loss
                    L3_TL = ((px_dot_TL + partial_x_TL) ** 2).mean()
                    L4_TL = ((py_dot_TL + partial_y_TL) ** 2).mean()

                    # Nota Bene: L1,L2,L3 and L4 are Hamilton's equations

                    # Initial conditions taken into consideration into the loss
                    ## Position
                    if parametrisation:
                        L5_TL = 0
                        L6_TL = 0
                        L7_TL = 0
                        L8_TL = 0
                    elif not parametrisation:
                        L5_TL = ((x_TL[0, 0] - initial_x) ** 2) * TL_weighting
                        L6_TL = (y_TL[0, 0] - initial_y_TL) ** 2
                        ## Velocity
                        L7_TL = (px_TL[0, 0] - 1) ** 2
                        L8_TL = (py_TL[0, 0] - 0) ** 2

                    # Could add the penalty that H is constant L9
                    L9_TL = ((H_0_TL - H_curr_TL) ** 2).mean()
                    if not energy_conservation:
                        # total loss
                        loss_TL += (
                            L1_TL
                            + L2_TL
                            + L3_TL
                            + L4_TL
                            + L5_TL
                            + L6_TL
                            + L7_TL
                            + L8_TL
                        )
                        # loss for current head
                        lossl_val_TL = (
                            L1_TL
                            + L2_TL
                            + L3_TL
                            + L4_TL
                            + L5_TL
                            + L6_TL
                            + L7_TL
                            + L8_TL
                        )
                    if energy_conservation:
                        # total loss
                        loss_TL += (
                            L1_TL
                            + L2_TL
                            + L3_TL
                            + L4_TL
                            + L5_TL
                            + L6_TL
                            + L7_TL
                            + L8_TL
                            + energy_TL_weight * L9_TL
                        )
                        # loss for current head
                        lossl_val_TL = (
                            L1_TL
                            + L2_TL
                            + L3_TL
                            + L4_TL
                            + L5_TL
                            + L6_TL
                            + L7_TL
                            + energy_TL_weight * L9_TL
                        )

                    # the loss for head l at epoch ne is stored
                    losses_part_TL[l][ne] = lossl_val_TL

                    # the loss for head l
                    losses_part_current_TL[l] = lossl_val_TL

                # Backward
                loss_TL.backward(retain_graph=True)

                # Here we perform clipping
                # (source: https://stackoverflow.com/questions/54716377/how-to-do-gradient-clipping-in-pytorch)
                """
            if norm_clipping:
              # Check that this is correct
              torch.nn.utils.clip_grad_norm_(network50.parameters(), max_norm=1000)
            """

                optimizer50.step()
                scheduler.step()
                tepoch_TL.set_postfix(loss=loss_TL.item())

                # the loss at epoch ne is stored
                # print("Updating the loss", loss_TL.item())
                loss_log_TL[ne] = loss_TL.item()

                # If it is the best loss so far, we update the best loss and saved the model
                if loss_TL.item() < temp_loss_TL:
                    epoch_mini_TL = ne + total_epochs_TL
                    network2_TL = copy.deepcopy(network50)
                    temp_loss_TL = loss_TL.item()
                    individual_losses_saved_TL = losses_part_current_TL

        try:
            print(
                "The best loss (for TL) we achieved was:",
                temp_loss_TL,
                "at epoch",
                epoch_mini_TL,
            )
        except UnboundLocalError:
            print("Increase number of epochs")

        maxi_indi_TL = 0
        for g in range(number_of_heads_TL):
            if individual_losses_saved_TL[g] > maxi_indi_TL:
                maxi_indi_TL = individual_losses_saved_TL[g]
        print(
            "The maximum of the individual losses (for TL) was {}".format(maxi_indi_TL)
        )
        total_epochs_TL += num_epochs_TL

        ### Save network2_TL here (to train again in the next cell) ######################
        torch.save(
            {
                "model_state_dict": network2_TL.state_dict(),
                "loss": temp_loss_TL,
                "epoch": epoch_mini_TL,
                "optimizer_state_dict": optimizer50.state_dict(),
                "total_epochs": total_epochs_TL,
                "initial_condition": ic_TL,
            },
            PATH,
        )
        ###############################################################################

        ########## Saving to a file  #####################################
        # Saving the network
        filename = (
            "Initial_x_"
            + str(initial_x)
            + "final_t_"
            + str(final_t)
            + "alpha_"
            + str(alpha_)
            + "width_"
            + str(width_)
            + "epochs_"
            + str(epochs_)
            + "epochsTL_"
            + str(epochs_TL)
            + "grid_size_"
            + str(grid_size)
            + "Network_state"
            + "TL"
            + ".p"
        )
        # os.mkdir(filename)
        f = open(filename, "wb")
        pickle.dump(network2_TL.state_dict(), f)
        f.close()
        #################################################################

        # Forward pass (network2 is the best network now)
        x_base_TL2 = network2_TL.base(t)
        d2_TL = network2_TL.forward_TL(x_base_TL2)

        # Plot the loss as a fct of the number of epochs
        ax[0].plot(range(num_epochs_TL), loss_log_TL, label="Total loss (for TL)")
        ax[0].set_title("Loss for TL")

        ########## Saving to a file  #####################################
        initial_y = ic_TL[0]
        # Saving the loss
        filename = (
            "Initial_x_"
            + str(initial_x)
            + "Initial_y_1_"
            + str(initial_y)
            + "final_t_"
            + str(final_t)
            + "alpha_"
            + str(alpha_)
            + "width_"
            + str(width_)
            + "epochs_"
            + str(epochs_)
            + "epochsTL_"
            + str(epochs_TL)
            + "grid_size_"
            + str(grid_size)
            + "loss"
            + "TL"
            + ".p"
        )
        # os.mkdir(filename)
        f = open(filename, "wb")
        pickle.dump(loss_log_TL, f)
        f.close()
        #################################################################

        # Now plot the individual trajectories and the individual losses
        for m in range(number_of_heads_TL):
            initial_y = ic_TL[m]
            # Get head m
            uf_TL = d2_TL[m]
            # The loss
            loss__TL = losses_part_TL[m]

            ########## Saving to a file  #####################################
            # Saving the trajectories
            filename = (
                "Head_"
                + str(m)
                + "Initial_x_"
                + str(initial_x)
                + "Initial_y_"
                + str(initial_y)
                + "final_t_"
                + str(final_t)
                + "alpha_"
                + str(alpha_)
                + "width_"
                + str(width_)
                + "epochs_"
                + str(epochs_)
                + "epochsTL_"
                + str(epochs_TL)
                + "grid_size_"
                + str(grid_size)
                + "Trajectory_NN_x"
                + "TL"
                + ".p"
            )
            # os.mkdir(filename)
            f = open(filename, "wb")
            pickle.dump(uf_TL.cpu().detach()[:, 0], f)
            f.close()
            # Saving the trajectories
            filename = (
                "Head_"
                + str(m)
                + "Initial_x_"
                + str(initial_x)
                + "Initial_y_"
                + str(initial_y)
                + "final_t_"
                + str(final_t)
                + "alpha_"
                + str(alpha_)
                + "width_"
                + str(width_)
                + "epochs_"
                + str(epochs_)
                + "epochsTL_"
                + str(epochs_TL)
                + "grid_size_"
                + str(grid_size)
                + "Trajectory_NN_y"
                + "TL"
                + ".p"
            )
            # os.mkdir(filename)
            f = open(filename, "wb")
            pickle.dump(uf_TL.cpu().detach()[:, 1], f)
            f.close()
            # Saving the trajectories
            filename = (
                "Head_"
                + str(m)
                + "Initial_x_"
                + str(initial_x)
                + "Initial_y_"
                + str(initial_y)
                + "final_t_"
                + str(final_t)
                + "alpha_"
                + str(alpha_)
                + "width_"
                + str(width_)
                + "epochs_"
                + str(epochs_)
                + "epochsTL_"
                + str(epochs_TL)
                + "grid_size_"
                + str(grid_size)
                + "Trajectory_NN_px"
                + "TL"
                + ".p"
            )
            # os.mkdir(filename)
            f = open(filename, "wb")
            pickle.dump(uf_TL.cpu().detach()[:, 2], f)
            f.close()
            # Saving the trajectories
            filename = (
                "Head_"
                + str(m)
                + "Initial_x_"
                + str(initial_x)
                + "Initial_y_"
                + str(initial_y)
                + "final_t_"
                + str(final_t)
                + "alpha_"
                + str(alpha_)
                + "width_"
                + str(width_)
                + "epochs_"
                + str(epochs_)
                + "epochsTL_"
                + str(epochs_TL)
                + "grid_size_"
                + str(grid_size)
                + "Trajectory_NN_py"
                + "TL"
                + ".p"
            )
            # os.mkdir(filename)
            f = open(filename, "wb")
            pickle.dump(uf_TL.cpu().detach()[:, 3], f)
            f.close()
            #################################################################

            if parametrisation:
                x_ = initial_x + (1 - torch.exp(-t)) * uf_TL[:, 0].reshape((-1, 1))
                y_ = initial_y + (1 - torch.exp(-t)) * uf_TL[:, 1].reshape((-1, 1))
                px_ = 1 + (1 - torch.exp(-t)) * uf_TL[:, 2].reshape((-1, 1))
                py_ = 0 + (1 - torch.exp(-t)) * uf_TL[:, 3].reshape((-1, 1))
                if print_legend:
                    ax[1].plot(
                        x_.cpu().detach(),
                        y_.cpu().detach(),
                        alpha=0.8,
                        ls=":",
                        label="NN solution (after TL) of SDE for {} head".format(m + 1),
                    )
                    t_p = np.linspace(-1, 1, 200)
                    ax[0].loglog(
                        range(num_epochs_TL),
                        loss__TL,
                        alpha=0.8,
                        label="{} component of the loss".format(m + 1),
                    )
                else:
                    ax[1].plot(x_.cpu().detach(), y_.cpu().detach(), alpha=0.8, ls=":")
                    t_p = np.linspace(-1, 1, 200)
                    ax[0].loglog(range(num_epochs_TL), loss__TL, alpha=0.8)

            elif not parametrisation:
                # Now we print the loss and the trajectory
                # We need to detach the tensors when working on GPU
                if print_legend:
                    ax[1].plot(
                        uf_TL.cpu().detach()[:, 0],
                        uf_TL.cpu().detach()[:, 1],
                        label="NN solution (after TL) of SDE for {} head".format(m + 1),
                    )
                    t_p = np.linspace(-1, 1, 200)
                    ax[0].loglog(
                        range(num_epochs_TL),
                        loss__TL,
                        alpha=0.8,
                        label="{} component of the loss".format(m + 1),
                    )
                else:
                    ax[1].plot(uf_TL.cpu().detach()[:, 0], uf_TL.cpu().detach()[:, 1])
                    t_p = np.linspace(-1, 1, 200)
                    ax[0].loglog(range(num_epochs_TL), loss__TL, alpha=0.8)

        # define the time
        Nt = 500
        t = np.linspace(0, final_t, Nt)

        t_comparaison = torch.linspace(0, final_t, Nt, requires_grad=True).reshape(
            -1, 1
        )

        # For the comparaison between the NN solution and the numerical solution,
        # we need to have the points at the same time
        # Set our tensor of times
        # t_comparaison=torch.linspace(0,final_t,Nt,requires_grad=True).reshape(-1,1)
        x_base_comparaison_TL = network2_TL.base(t_comparaison)
        d_comparaison_TL = network2_TL.forward_TL(x_base_comparaison_TL)

        # Initial positon and velocity
        x0, px0, py0 = 0, 1, 0.0
        # Initial y position
        Y0_TL = ic_TL

        # Maximum and mim=nimum x at final time
        maximum_x = initial_x
        maximum_y = 0
        minimum_y = 0
        min_final = np.inf

        for i in range(number_of_heads_TL):
            initial_y = Y0_TL[i]
            print("The initial condition used is", Y0_TL[i])
            x, y, px, py = rayTracing_general(
                t, x0, Y0_TL[i], px0, py0, means_cell, sig=sig, A_=A_
            )
            if x[-1] > maximum_x:
                maximum_x = x[-1]
            if x[-1] < min_final:
                min_final = x[-1]
            if min(y) < minimum_y:
                minimum_y = min(y)
            if max(y) > maximum_y:
                maximum_y = max(y)

            ########## Saving ###########
            # Saving the (numerical trajectories)
            filename = (
                "Initial_x_"
                + str(initial_x)
                + "final_t_"
                + str(final_t)
                + "alpha_"
                + str(alpha_)
                + "width_"
                + str(width_)
                + "epochs_"
                + str(epochs_)
                + "grid_size_"
                + str(grid_size)
                + "Trajectories_x_TL"
                + ".p"
            )
            # os.mkdir(filename)
            f = open(filename, "wb")
            pickle.dump(x, f)
            f.close()
            # Saving the (numerical trajectories)
            filename = (
                "Initial_x_"
                + str(initial_x)
                + "final_t_"
                + str(final_t)
                + "alpha_"
                + str(alpha_)
                + "width_"
                + str(width_)
                + "epochs_"
                + str(epochs_)
                + "grid_size_"
                + str(grid_size)
                + "Trajectories_y_TL"
                + ".p"
            )
            # os.mkdir(filename)
            f = open(filename, "wb")
            pickle.dump(y, f)
            f.close()
            # Saving the (numerical trajectories)
            filename = (
                "Initial_x_"
                + str(initial_x)
                + "final_t_"
                + str(final_t)
                + "alpha_"
                + str(alpha_)
                + "width_"
                + str(width_)
                + "epochs_"
                + str(epochs_)
                + "grid_size_"
                + str(grid_size)
                + "Trajectories_px_TL"
                + ".p"
            )
            # os.mkdir(filename)
            f = open(filename, "wb")
            pickle.dump(px, f)
            f.close()
            # Saving the (numerical trajectories)
            filename = (
                "Initial_x_"
                + str(initial_x)
                + "final_t_"
                + str(final_t)
                + "alpha_"
                + str(alpha_)
                + "width_"
                + str(width_)
                + "epochs_"
                + str(epochs_)
                + "grid_size_"
                + str(grid_size)
                + "Trajectories_py_TL"
                + ".p"
            )
            # os.mkdir(filename)
            f = open(filename, "wb")
            pickle.dump(py, f)
            f.close()
            #############################

            ax[1].plot(x, y, "g", linestyle=":", linewidth=lineW)

            # Comparaison
            # Get head m
            trajectoires_xy_TL = d_comparaison_TL[i]

            if parametrisation:
                print("Initial x is {}", initial_x)
                print("Initial y is {}", initial_y)
                x_ = initial_x + (1 - torch.exp(-t_comparaison)) * trajectoires_xy_TL[
                    :, 0
                ].reshape((-1, 1))
                y_ = initial_y + (1 - torch.exp(-t_comparaison)) * trajectoires_xy_TL[
                    :, 1
                ].reshape((-1, 1))
                px_ = 1 + (1 - torch.exp(-t_comparaison)) * trajectoires_xy_TL[
                    :, 2
                ].reshape((-1, 1))
                py_ = 0 + (1 - torch.exp(-t_comparaison)) * trajectoires_xy_TL[
                    :, 3
                ].reshape((-1, 1))

                # MSE:
                MSE_TL = (
                    (x_.cpu().detach().reshape((-1, 1)) - x.reshape((-1, 1))) ** 2
                ).mean() + (
                    (y_.cpu().detach().reshape((-1, 1)) - y.reshape((-1, 1))) ** 2
                ).mean()
                MSE_TL += (
                    (px_.cpu().detach().reshape((-1, 1)) - px.reshape((-1, 1))) ** 2
                ).mean() + (
                    (py_.cpu().detach().reshape((-1, 1)) - py.reshape((-1, 1))) ** 2
                ).mean()
                MSE_TL = MSE_TL / (4 * Nt)
                # Should probably do a dict that saves them / save them to a file for the cluster
                print("The MSE for head {} is {}".format(i, MSE_TL))

                px_comparaison_TL = px_
                py_comparaison_TL = py_
                x_comparaison_TL = x_
                y_comparaison_TL = y_

            elif not parametrisation:
                # MSE:
                MSE_TL = (
                    (
                        trajectoires_xy_TL.cpu().detach()[:, 0].reshape((-1, 1))
                        - x.reshape((-1, 1))
                    )
                    ** 2
                ).mean() + (
                    (
                        trajectoires_xy_TL.cpu().detach()[:, 1].reshape((-1, 1))
                        - y.reshape((-1, 1))
                    )
                    ** 2
                ).mean()
                MSE_TL += (
                    (
                        trajectoires_xy_TL.cpu().detach()[:, 2].reshape((-1, 1))
                        - px.reshape((-1, 1))
                    )
                    ** 2
                ).mean() + (
                    (
                        trajectoires_xy_TL.cpu().detach()[:, 3].reshape((-1, 1))
                        - py.reshape((-1, 1))
                    )
                    ** 2
                ).mean()
                MSE_TL = MSE_TL / (4 * Nt)
                # Should probably do a dict that saves them / save them to a file for the cluster
                print("The MSE (TL) for head {} is {}".format(i, MSE_TL))
                # Is there a way to print it for every epoch like Blake? Yes, but more expensive. I
                # think Blake should also actually consider not computing it for every epoch
                # much more efficient

                # Compute the energy along t_comparaison
                px_comparaison_TL = trajectoires_xy_TL[:, 2]
                py_comparaison_TL = trajectoires_xy_TL[:, 3]
                px_comparaison_TL = px_comparaison_TL.reshape((-1, 1))
                py_comparaison_TL = py_comparaison_TL.reshape((-1, 1))

                x_comparaison_TL = trajectoires_xy_TL[:, 0]
                y_comparaison_TL = trajectoires_xy_TL[:, 1]
                x_comparaison_TL = x_comparaison_TL.reshape((-1, 1))
                y_comparaison_TL = y_comparaison_TL.reshape((-1, 1))

            # Theoretical energy
            print("The theoretical energy is {}".format(H0_init_TL[i]))
            ax[2].plot(
                t_comparaison.cpu().detach(),
                H0_init_TL[i] * np.ones(Nt),
                linestyle=":",
                c="r",
            )
            ax[2].set_title("Energy (TL)")

            H_curr_comparaison_TL = (
                px_comparaison_TL**2 + py_comparaison_TL**2
            ) / 2
            for m in range(len(means)):
                # Get the current means
                mu_x = means[m][0]
                mu_y = means[m][1]

                # Updating the energy
                H_curr_comparaison_TL += -A_ * torch.exp(
                    -(1 / (2 * sig**2))
                    * ((x_comparaison_TL - mu_x) ** 2 + (y_comparaison_TL - mu_y) ** 2)
                )
            ax[2].plot(
                t_comparaison.cpu().detach(), H_curr_comparaison_TL.cpu().detach()
            )

        y1 = np.linspace(-0.1, 1.1, 500)
        x1 = np.linspace(-0.1, 1.1, 500)
        x, y = np.meshgrid(x1, y1)

        V = 0
        Vx = 0
        Vy = 0

        A_ = 0.1
        sig = 0.1

        for i in means_cell:
            muX1 = i[0]
            muY1 = i[1]
            V += -A_ * np.exp(-(((x - muX1) ** 2 + (y - muY1) ** 2) / sig**2) / 2)

        ax[1].contourf(x1, y1, V, levels=20, cmap="Reds_r")
        # ax[0].colorbar()
        ax[1].set_xlim(-0.1, 1.1)
        t1_TL = time.time()

        filename_fig = (
            "Initial_x_"
            + str(initial_x)
            + "Initial_y_"
            + str(initial_y)
            + "final_t_"
            + str(final_t)
            + "alpha_"
            + str(alpha_)
            + "width_"
            + str(width_)
            + "epochs_"
            + str(epochs_)
            + "grid_size_"
            + str(grid_size)
            + "Trajectories_px_TL"
            + ".png"
        )
        plt.savefig(filename_fig)

        print("For TL, we had {} head".format(number_of_heads_TL))
        print("Total time for TL is {}".format(t1_TL - t0_TL))


for i in range(1):
    filename_saved = "../OInitial_x_0final_t_1alpha_1width_40epochs_25000grid_size_400Network_state.pth"
    network_base = MyNetwork_Ray_Tracing(
        number_dims=40, number_dims_heads=10, N=11, Number_heads_TL=1
    )
    network_base.load_state_dict(
        torch.load(filename_saved, map_location=torch.device("cpu"))
    )

    TL_N_heads_run_Gaussiann_transfer(
        network_base,
        specify_initial_condition=True,
        init_specified=[0.55],
        step_LR_step=1000,
        step_LR_gamma=0.995,
        sgd_lr=0.025,
        use_SGD_TL=True,
        parametrisation=True,
        max_grid_grow=400,
        epochs_=25500,
        epochs_TL=25000,
        grid_size=400,
        number_of_heads=11,
        number_of_heads_TL=1,
    )
