"""
Paper.ipynb
Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wh2vOyq2gVt9n6kI2lx2nwCwon6VfU4z
(now modified)
"""

# Imports
import copy
import math
import pickle
import random
import time

import click
import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.optim as optim
from tqdm import trange

from AD import diff
from neural_network_architecture import NeuralNetwork
from reparametrize import reparametrize
from plot import plot_all, potential_grid
from params import means_cell

# Use GPU if possible
if torch.cuda.is_available():
    device = torch.device("cuda")
    torch.set_default_tensor_type(torch.cuda.DoubleTensor)
    print("Using GPU")
else:
    device = torch.device("cpu")
    torch.set_default_tensor_type(torch.DoubleTensor)
    print("No GPU found, using cpu")


lineW: int = 3
lineBoxW: int = 2
font: dict = {"size": 24}

plt.rc("font", **font)


font = {"size": 24}

plt.rc("font", **font)



def initial_full_network_training(
    random_ic: bool = True,
    parametrisation: bool = False,
    scale: float = 0.7,
    alpha_: float = 0.1,
    sigma: float = 0.1,
    initial_x: float = 0,
    final_t: float = 1,
    means_cell: list = means_cell,
    width_base: int = 40,
    width_heads: int = 10,
    number_of_epochs: int = 25000,
    grid_size: int = 400,
    number_of_heads: int = 11,
    number_of_heads_TL: int = 1,
    PATH: str = "models",
    load_weights: bool = False,
    energy_conservation: bool = False,
    norm_clipping: bool = False,
) -> NeuralNetwork:
    """

    Args:
        means_cell should be of the forms [[mu_x1,mu_y1],..., [mu_xn,mu_yn]]

        random_ic:
            whether we have random initial conditions within the possible y(0) values.
            Otherwise, we divide the [0,1] interval into (width_heads-1) intervals and place
            the initial conditions for y at each end.
        parametrisation:
            whether the output of the NN is parametrized to satisfy the boundary conditions exactly or
            whether that should be a component of the loss.
        scale:
        alpha_:
        sigma:
        initial_x:
            inital value for x(0)
        final_t:
            final time
        means_cell:
            the means used for tge Gaussians
        alpha_:
        width_base:
            the width of the base
        width_heads:
            the width of each head
        number_of_epochs:
            the number of epochs we train the NN for
        grid_size:
            the number of random points (time) used in training
        number_of_heads:
            the number of heads
        number_of_heads_TL:
        PATH:
        print_legend:
        load_weights:
        energy_conservation:
            whether to add an energy conservation loss to the total loss
        norm_clipping:
    """
    # We will time the process
    # Access the current time
    t0 = time.time()

    # Set out tensor of times
    t = torch.linspace(0, final_t, grid_size, requires_grad=True).reshape(-1, 1)

    # We keep a log of the loss as a fct of the number of epochs
    loss_record = np.zeros(number_of_epochs)

    # For comparaison
    temporary_loss = np.inf

    t0_initial = time.time()

    # Set up the network
    network = NeuralNetwork(
        width_base=width_base,
        number_dims_heads=width_heads,
        number_heads=number_of_heads,
        number_heads_tl=number_of_heads_TL,
    )
    # Make a deep copy
    network2 = copy.deepcopy(network)

    # optimizer. Would be fun to play more with this.
    optimizer = optim.Adam(network.parameters(), lr=1e-3)
    # scheduler for the optimizer
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=scale)

    # Dictionary for the initial conditions
    initial_conditions_dictionary: dict = {}
    # Dictionary for the initial energy for each initial conditions
    H0_init: dict = {}

    # Random create initial conditions
    if not load_weights:
        if random_ic:
            for j in range(number_of_heads):
                # Initial conditions
                initial_condition = random.randint(0, 100) / 100
                print("The initial condition (for y) is {}".format(initial_condition))
                initial_conditions_dictionary[j] = initial_condition
        else:
            a = np.linspace(0, 1, number_of_heads)
            for j in range(number_of_heads):
                initial_conditions_dictionary[j] = a[j]

    # Keep track of the number of epochs
    total_epochs: int = 0

    ## LOADING WEIGHTS PART if PATH file exists and load_weights=True
    if load_weights == True:
        print("We loaded the previous model")
        checkpoint = torch.load(PATH)
        device = torch.device("cuda")
        network.load_state_dict(checkpoint["model_state_dict"])
        optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
        network.to(device)
        total_epochs += checkpoint["total_epochs"]
        initial_conditions_dictionary = checkpoint["initial_condition"]
        print("We previously trained for {} epochs".format(total_epochs))
        print(
            "The loss was:",
            checkpoint["loss"],
            "achieved at epoch",
            checkpoint["epoch"],
        )

    # Dictionary keeping track of the loss for each head
    losses_each_head: dict = {}
    for k in range(number_of_heads):
        losses_each_head[k] = np.zeros(number_of_epochs)

    # For every epoch...
    with trange(number_of_epochs) as tepoch:
        for ne in tepoch:
            tepoch.set_description(f"Epoch {ne}")
            optimizer.zero_grad()
            # Random sampling
            t = torch.rand(grid_size, requires_grad=True) * final_t
            t, _ = torch.sort(t)
            t[0] = 0
            t = t.reshape(-1, 1)
            # Forward pass through the network
            x_base = network.base(t)
            d = network.forward_initial(x_base)
            # loss
            loss = 0
            # for saving the best loss (of individual heads)
            losses_each_head_current = {}

            # For each head...
            for l in range(number_of_heads):
                # Get the current head
                head = d[l]
                # Get the corresponding initial condition
                initial_y = initial_conditions_dictionary[l]

                # Outputs
                if parametrisation:
                    x, y, px, py = reparametrize(
                        initial_x=initial_x, initial_y=initial_y, t=t, head=head
                    )
                elif not parametrisation:
                    x = head[:, 0]
                    y = head[:, 1]
                    px = head[:, 2]
                    py = head[:, 3]
                x = x.reshape((-1, 1))
                y = y.reshape((-1, 1))
                px = px.reshape((-1, 1))
                py = py.reshape((-1, 1))
                # Derivatives
                x_dot = diff(x, t, 1)
                y_dot = diff(y, t, 1)
                px_dot = diff(px, t, 1)
                py_dot = diff(py, t, 1)

                # Loss
                L1 = ((x_dot - px) ** 2).mean()
                L2 = ((y_dot - py) ** 2).mean()

                # For the other components of the loss, we need the potential V
                # and its derivatives
                ## Partial derivatives of the potential (updated below)
                partial_x = 0
                partial_y = 0

                ## Energy at the initial time (updated below)
                ## H0=1/2-potential evaluated at (x0, y0) ie (px0**2+py0**2)/2 - potential evaluated at (x0,y0)
                ## H_curr=(px**2+py**2)/2-potential evaluated at (x,y)
                H_0 = 1 / 2
                H_curr = (px**2 + py**2) / 2

                for i in range(len(means_cell)):
                    # Get the current means_cell
                    mu_x = means_cell[i][0]
                    mu_y = means_cell[i][1]

                    # Building the potential and updating the partial derivatives
                    potential = -alpha_ * torch.exp(
                        -(1 / (2 * sigma**2)) * ((x - mu_x) ** 2 + (y - mu_y) ** 2)
                    )
                    # Partial wrt to x
                    partial_x += -potential * (x - mu_x) * (1 / (sigma**2))
                    # Partial wrt to y
                    partial_y += -potential * (y - mu_y) * (1 / (sigma**2))

                    # Updating the energy
                    H_0 += -alpha_ * math.exp(
                        -(1 / (2 * sigma**2))
                        * ((initial_x - mu_x) ** 2 + (initial_y - mu_y) ** 2)
                    )
                    H_curr += -alpha_ * torch.exp(
                        -(1 / (2 * sigma**2)) * ((x - mu_x) ** 2 + (y - mu_y) ** 2)
                    )

                ## We can finally set the energy for head l
                H0_init[l] = H_0

                # Other components of the loss
                L3 = ((px_dot + partial_x) ** 2).mean()
                L4 = ((py_dot + partial_y) ** 2).mean()

                # Nota Bene: L1,L2,L3 and L4 are Hamilton's equations

                # Initial conditions taken into consideration into the loss
                ## Position
                if parametrisation:
                    L5 = 0
                    L6 = 0
                    L7 = 0
                    L8 = 0
                elif not parametrisation:
                    L5 = (x[0, 0] - initial_x) ** 2
                    L6 = (y[0, 0] - initial_y) ** 2
                    ## Velocity
                    L7 = (px[0, 0] - 1) ** 2
                    L8 = (py[0, 0] - 0) ** 2

                # Could add the penalty that H is constant L9
                L9 = ((H_0 - H_curr) ** 2).mean()
                if not energy_conservation:
                    # total loss
                    loss += L1 + L2 + L3 + L4 + L5 + L6 + L7 + L8
                    # loss for current head
                    lossl_val = L1 + L2 + L3 + L4 + L5 + L6 + L7 + L8
                if energy_conservation:
                    # total loss
                    loss += L1 + L2 + L3 + L4 + L5 + L6 + L7 + L8 + L9
                    # loss for current head
                    lossl_val = L1 + L2 + L3 + L4 + L5 + L6 + L7 + L8 + L9

                # the loss for head l at epoch ne is stored
                losses_each_head[l][ne] = lossl_val

                # the loss for head l
                losses_each_head_current[l] = lossl_val

            # Backward
            loss.backward()

            # Here we perform clipping
            # (source: https://stackoverflow.com/questions/54716377/how-to-do-gradient-clipping-in-pytorch)
            if norm_clipping:
                torch.nn.utils.clip_grad_norm_(network.parameters(), max_norm=1000)

            optimizer.step()
            scheduler.step()
            tepoch.set_postfix(loss=loss.item())

            # the loss at epoch ne is stored
            loss_record[ne] = loss.item()

            # If it is the best loss so far, we update the best loss and saved the model
            if loss.item() < temporary_loss:
                epoch_mini = ne + total_epochs
                network2 = copy.deepcopy(network)
                temporary_loss = loss.item()
                individual_losses_saved = losses_each_head_current
    try:
        print("The best loss we achieved was:", temporary_loss, "at epoch", epoch_mini)
    except UnboundLocalError:
        print("Increase number of epochs")

    maxi_indi = 0
    for g in range(number_of_heads):
        if individual_losses_saved[g] > maxi_indi:
            maxi_indi = individual_losses_saved[g]
    print("The maximum of the individual losses was {}".format(maxi_indi))
    total_epochs += number_of_epochs

    ### Save network2 here (to train again in the next cell) ######################
    torch.save(
        {
            "model_state_dict": network2.state_dict(),
            "loss": temporary_loss,
            "epoch": epoch_mini,
            "optimizer_state_dict": optimizer.state_dict(),
            "total_epochs": total_epochs,
            "initial_condition": initial_conditions_dictionary,
        },
        PATH,
    )
    ###############################################################################

    # Forward pass (network2 is the best network now)
    x_base2 = network2.base(t)
    d2 = network2.forward_initial(x_base2)

    t1_initial = time.time()

    print(
        "The elapsed time (for the first, initial training) is {}".format(
            t1_initial - t0_initial
        )
    )

    # Saving the initial conditions
    filename = f"Data/Initial_x_{str(initial_x)}_final_t_{str(final_t)}_alpha_{str(alpha_)}_width_base_{str(width_base)}_number_of_epochs_{str(number_of_epochs)}_grid_size_{str(grid_size)}_Initial_conditions.p"
    f = open(filename, "wb")
    pickle.dump(initial_conditions_dictionary, f)
    f.close()

    return network2, d2, loss_record, losses_each_head, initial_conditions_dictionary, H0_init



####################################################################################################
######################################## Main ######################################################
####################################################################################################

# finish setting up all the clicks with the inputs
# to initial_full_network_training and pass in
# to plot and potential grid as well


@click.command()
# @click.option(
#     "ne",
#     "number_of_epochs",
#     default=2500,
#     help="Number of epochs we use to train the NN",
# )
# @click.option("nh", "number_of_heads", default=11, help="Number of heads")
# @click.option("ft", "final_time", default=1, help="Final time")
# @click.option("wba", "width_base", default=40, help="Width of the base")
def main(
    number_of_epochs : int = 25 , number_of_heads : int = 11, final_time : float = 1 , width_base : int = 40,
):
    initial_x = 0
    alpha_ = 1
    grid_size = 400
    sigma = 0.1
    parametrisation = True

    network_base, d2, loss_record, losses_each_head, initial_conditions_dictionary, H0_init= initial_full_network_training(
        random_ic=False,
        parametrisation= parametrisation,
        energy_conservation=False,
        number_of_epochs=number_of_epochs,
        grid_size=grid_size,
        number_of_heads=number_of_heads,
        number_of_heads_TL=1,
        alpha_=alpha_,
        sigma = sigma,
    )

    # Saving the network
    filename = f"Data/Initial_x_{str(initial_x)}_final_t_{str(final_time)}_alpha_{str(alpha_)}_width_base_{str(width_base)}_number_of_epochs{str(number_of_epochs)}_grid_size_{str(grid_size)})_network_state.pth"
    # os.mkdir(filename)
    f = open(filename, "wb")
    torch.save(network_base.state_dict(), f)
    f.close()

    # Saving the loss
    filename = f"Data/Initial_x_{str(initial_x)}_final_t_{str(final_time)}_alpha_{str(alpha_)}_width_base_{str(width_base)}_number_of_epochs_{str(number_of_epochs)}_grid_size_{str(grid_size)}_loss.p"
    f = open(filename, "wb")
    pickle.dump(loss_record, f)
    f.close()

    x1, y1, V = potential_grid(
        initial_x=initial_x, final_t=final_time, alpha_=alpha_, means_cell=means_cell, sigma=sigma
    )

    plot_all(number_of_epochs = number_of_epochs, 
         number_of_heads = number_of_heads,
         loss_record = loss_record,
         losses_each_head = losses_each_head,
         network_trained = network_base,
         d2 = d2,
         parametrisation = parametrisation,
         initial_conditions_dictionary=initial_conditions_dictionary,
         initial_x=initial_x,
         final_t = final_time,
         width_base =width_base,
         alpha_ = alpha_,
         grid_size = grid_size,
         x1=x1,
         y1=y1,
         V=V,
         sig=sigma,
         H0_init=H0_init,
         print_legend= True,
         )


if __name__ == "__main__":
    main()


# Didn't keep any of the TL code from the colab (in Paper folder)
